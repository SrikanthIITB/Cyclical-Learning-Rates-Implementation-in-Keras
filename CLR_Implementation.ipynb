{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xAhTPfKO7ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9f543471-1fc0-4444-b9ce-fcbb3ae220c8"
      },
      "source": [
        "'''from zipfile import ZipFile\n",
        "file_name = '/content/keras-cyclical-learning-rates.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"from zipfile import ZipFile\\nfile_name = '/content/keras-cyclical-learning-rates.zip'\\n\\nwith ZipFile(file_name, 'r') as zip:\\n  zip.extractall()\\n  print('Done')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbm_sJkwPpk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "import os\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# initialize the list of class label names\n",
        "CLASSES = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\",\n",
        "\t\"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "# define the minimum learning rate, maximum learning rate, batch size,\n",
        "# step size, CLR method, and number of epochs\n",
        "MIN_LR = 1e-7\n",
        "MAX_LR = 1e-2\n",
        "BATCH_SIZE = 64\n",
        "STEP_SIZE = 8\n",
        "CLR_METHOD = \"triangular\"\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# define the path to the output training history plot and cyclical\n",
        "# learning rate plot\n",
        "TRAINING_PLOT_PATH = os.path.sep.join([\"/content/sample_data\", \"training_plot.png\"])\n",
        "CLR_PLOT_PATH = os.path.sep.join([\"/content/sample_data\", \"clr_plot.png\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAaVleTvQg0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import *\n",
        "import numpy as np\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8oh4rksQsKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class MiniGoogLeNet:\n",
        "\t@staticmethod\n",
        "\tdef conv_module(x, K, kX, kY, stride, chanDim, padding=\"same\"):\n",
        "\t\t# define a CONV => BN => RELU pattern\n",
        "\t\tx = Conv2D(K, (kX, kY), strides=stride, padding=padding)(x)\n",
        "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\n",
        "\t\treturn x\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef inception_module(x, numK1x1, numK3x3, chanDim):\n",
        "\t\tconv_1x1 = MiniGoogLeNet.conv_module(x, numK1x1, 1, 1,\n",
        "\t\t\t(1, 1), chanDim)\n",
        "\t\tconv_3x3 = MiniGoogLeNet.conv_module(x, numK3x3, 3, 3,\n",
        "\t\t\t(1, 1), chanDim)\n",
        "\t\tx = concatenate([conv_1x1, conv_3x3], axis=chanDim)\n",
        "\t\treturn x\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef downsample_module(x, K, chanDim):\n",
        "\t\tconv_3x3 = MiniGoogLeNet.conv_module(x, K, 3, 3, (2, 2),\n",
        "\t\t\tchanDim, padding=\"valid\")\n",
        "\t\tpool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\t\tx = concatenate([conv_3x3, pool], axis=chanDim)\n",
        "\t\treturn x\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes):\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "\t\tinputs = Input(shape=inputShape)\n",
        "\t\tx = MiniGoogLeNet.conv_module(inputs, 96, 3, 3, (1, 1),\n",
        "\t\t\tchanDim)\n",
        "\n",
        "\t\t# two Inception modules followed by a downsample module\n",
        "\t\tx = MiniGoogLeNet.inception_module(x, 32, 32, chanDim)\n",
        "\t\tx = MiniGoogLeNet.inception_module(x, 32, 48, chanDim)\n",
        "\t\tx = MiniGoogLeNet.downsample_module(x, 80, chanDim)\n",
        "\n",
        "\t\tx = MiniGoogLeNet.inception_module(x, 112, 48, chanDim)\n",
        "\t\tx = MiniGoogLeNet.inception_module(x, 96, 64, chanDim)\n",
        "\t\tx = MiniGoogLeNet.inception_module(x, 80, 80, chanDim)\n",
        "\t\tx = MiniGoogLeNet.inception_module(x, 48, 96, chanDim)\n",
        "\t\tx = MiniGoogLeNet.downsample_module(x, 96, chanDim)\n",
        "\n",
        "\t\tx = MiniGoogLeNet.inception_module(x, 176, 160, chanDim)\n",
        "\t\tx = MiniGoogLeNet.inception_module(x, 176, 160, chanDim)\n",
        "\t\tx = AveragePooling2D((7, 7))(x)\n",
        "\t\tx = Dropout(0.5)(x)\n",
        "\n",
        "\t\t# softmax classifier\n",
        "\t\tx = Flatten()(x)\n",
        "\t\tx = Dense(classes)(x)\n",
        "\t\tx = Activation(\"softmax\")(x)\n",
        "\n",
        "\t\tmodel = Model(inputs, x, name=\"googlenet\")\n",
        "\t\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmQqgbxUQzT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "outputId": "bb6355d1-8686-49c3-dae4-cb84ba461726"
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"[INFO] loading CIFAR-10 data...\")\n",
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
        "trainX = trainX.astype(\"float\")\n",
        "testX = testX.astype(\"float\")\n",
        "\n",
        "# apply mean subtraction to the data\n",
        "mean = np.mean(trainX, axis=0)\n",
        "trainX -= mean\n",
        "testX -= mean\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)\n",
        "\n",
        "# construct the image generator for data augmentation\n",
        "aug = ImageDataGenerator(width_shift_range=0.1,\n",
        "\theight_shift_range=0.1, horizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "\n",
        "# initialize the optimizer and model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = SGD(lr=MIN_LR, momentum=0.9)\n",
        "model = MiniGoogLeNet.build(width=32, height=32, depth=3, classes=10)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# initialize the cyclical learning rate callback\n",
        "print(\"[INFO] using '{}' method\".format(CLR_METHOD))\n",
        "clr = CyclicLR(\n",
        "\tmode=CLR_METHOD,\n",
        "\tbase_lr=MIN_LR,\n",
        "\tmax_lr=MAX_LR,\n",
        "\tstep_size= STEP_SIZE * (trainX.shape[0] // BATCH_SIZE))\n",
        "\n",
        "print(\"[INFO] training network...\")\n",
        "H = model.fit(\n",
        "\tx=aug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tsteps_per_epoch=trainX.shape[0] // BATCH_SIZE,\n",
        "\tepochs=NUM_EPOCHS,\n",
        "\tcallbacks=[clr],\n",
        "\tverbose=1)\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(x=testX, batch_size=BATCH_SIZE)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=CLASSES))\n",
        "\n",
        "N = np.arange(0, NUM_EPOCHS)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(TRAINING_PLOT_PATH)\n",
        "\n",
        "# plot the learning rate history\n",
        "N = np.arange(0, len(clr.history[\"lr\"]))\n",
        "plt.figure()\n",
        "plt.plot(N, clr.history[\"lr\"])\n",
        "plt.title(\"Cyclical Learning Rate (CLR)\")\n",
        "plt.xlabel(\"Training Iterations\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.savefig(CLR_PLOT_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading CIFAR-10 data...\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "[INFO] compiling model...\n",
            "[INFO] using 'triangular' method\n",
            "[INFO] training network...\n",
            "Epoch 1/10\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.9493 - accuracy: 0.2856 - val_loss: 1.5211 - val_accuracy: 0.4311\n",
            "Epoch 2/10\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.4141 - accuracy: 0.4833 - val_loss: 1.2169 - val_accuracy: 0.5657\n",
            "Epoch 3/10\n",
            "781/781 [==============================] - 36s 45ms/step - loss: 1.1447 - accuracy: 0.5914 - val_loss: 1.1179 - val_accuracy: 0.6006\n",
            "Epoch 4/10\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.9783 - accuracy: 0.6559 - val_loss: 1.2335 - val_accuracy: 0.6199\n",
            "Epoch 5/10\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.8539 - accuracy: 0.7033 - val_loss: 0.9156 - val_accuracy: 0.6749\n",
            "Epoch 6/10\n",
            "781/781 [==============================] - 36s 45ms/step - loss: 0.7663 - accuracy: 0.7357 - val_loss: 1.0689 - val_accuracy: 0.6738\n",
            "Epoch 7/10\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.7016 - accuracy: 0.7575 - val_loss: 0.7813 - val_accuracy: 0.7423\n",
            "Epoch 8/10\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.6411 - accuracy: 0.7793 - val_loss: 0.7259 - val_accuracy: 0.7606\n",
            "Epoch 9/10\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.5773 - accuracy: 0.8027 - val_loss: 0.7721 - val_accuracy: 0.7433\n",
            "Epoch 10/10\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.5152 - accuracy: 0.8254 - val_loss: 0.5525 - val_accuracy: 0.8114\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.89      0.78      0.83      1000\n",
            "  automobile       0.89      0.95      0.92      1000\n",
            "        bird       0.82      0.68      0.74      1000\n",
            "         cat       0.69      0.74      0.72      1000\n",
            "        deer       0.80      0.79      0.80      1000\n",
            "         dog       0.90      0.58      0.71      1000\n",
            "        frog       0.72      0.94      0.82      1000\n",
            "       horse       0.68      0.94      0.79      1000\n",
            "        ship       0.92      0.90      0.91      1000\n",
            "       truck       0.96      0.80      0.87      1000\n",
            "\n",
            "    accuracy                           0.81     10000\n",
            "   macro avg       0.83      0.81      0.81     10000\n",
            "weighted avg       0.83      0.81      0.81     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWCmOZZK4xB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "11df0b07-44f4-4958-9bf6-37c5201cb485"
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"[INFO] loading CIFAR-10 data...\")\n",
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
        "trainX = trainX.astype(\"float\")\n",
        "testX = testX.astype(\"float\")\n",
        "\n",
        "mean = np.mean(trainX, axis=0)\n",
        "trainX -= mean\n",
        "testX -= mean\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)\n",
        "aug = ImageDataGenerator(width_shift_range=0.1,\n",
        "\theight_shift_range=0.1, horizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = SGD(lr=MIN_LR, momentum=0.9)\n",
        "model = MiniGoogLeNet.build(width=32, height=32, depth=3, classes=10)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "print(\"[INFO] using '{}' method\".format('Linear Learning Rate'))\n",
        "clr2 = CyclicLR(\n",
        "\tmode=Linear_METHOD,\n",
        "\tbase_lr=MIN_LR,\n",
        "\tmax_lr=MAX_LR,\n",
        "\tstep_size= STEP_SIZE * (trainX.shape[0] // BATCH_SIZE))\n",
        "print(\"[INFO] training network...\")\n",
        "H = model.fit(\n",
        "\tx=aug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tsteps_per_epoch=trainX.shape[0] // BATCH_SIZE,\n",
        "\tepochs=NUM_EPOCHS,\n",
        "  callbacks=clr2,\n",
        "\tverbose=1)\n",
        "\n",
        "# evaluate the network and show a classification report\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(x=testX, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading CIFAR-10 data...\n",
            "[INFO] compiling model...\n",
            "[INFO] using 'Linear Learning Rate' method\n",
            "[INFO] training network...\n",
            "Epoch 1/10\n",
            "781/781 [==============================] - 37s 47ms/step - loss: 1.9722 - accuracy: 0.2741 - val_loss: 1.4618 - val_accuracy: 0.1747\n",
            "Epoch 2/10\n",
            "781/781 [==============================] - 37s 48ms/step - loss: 1.4385 - accuracy: 0.4759 - val_loss: 1.7933 - val_accuracy: 0.2608\n",
            "Epoch 3/10\n",
            "781/781 [==============================] - 37s 47ms/step - loss: 1.1841 - accuracy: 0.5740 - val_loss: 1.4227 - val_accuracy: 0.3370\n",
            "Epoch 4/10\n",
            "781/781 [==============================] - 37s 48ms/step - loss: 0.9945 - accuracy: 0.6485 - val_loss: 1.0725 - val_accuracy: 0.4527\n",
            "Epoch 5/10\n",
            "781/781 [==============================] - 37s 48ms/step - loss: 0.8692 - accuracy: 0.6962 - val_loss: 1.1031 - val_accuracy: 0.4383\n",
            "Epoch 6/10\n",
            "781/781 [==============================] - 37s 48ms/step - loss: 0.7723 - accuracy: 0.7336 - val_loss: 0.9561 - val_accuracy: 0.4854\n",
            "Epoch 7/10\n",
            "781/781 [==============================] - 38s 48ms/step - loss: 0.6957 - accuracy: 0.7587 - val_loss: 0.7837 - val_accuracy: 0.5379\n",
            "Epoch 8/10\n",
            "781/781 [==============================] - 38s 48ms/step - loss: 0.6415 - accuracy: 0.7799 - val_loss: 0.8996 - val_accuracy: 0.5052\n",
            "Epoch 9/10\n",
            "781/781 [==============================] - 37s 48ms/step - loss: 0.5787 - accuracy: 0.8027 - val_loss: 0.8342 - val_accuracy: 0.5408\n",
            "Epoch 10/10\n",
            "781/781 [==============================] - 37s 48ms/step - loss: 0.5101 - accuracy: 0.8263 - val_loss: 0.8174 - val_accuracy: 0.5559\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}